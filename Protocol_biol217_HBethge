# Protocol_bio217_Hendrik Bethge 
## 1.Metagenome assembled Genomes (MAGs)
The goal of this project was to learn how to handle and interprete metagenomic data from the raw read to the assembled MAGs. 

This process consists of five steps:
 1. Pre-processing of the raw reads 
 2. Assembled reads into contigs/fasta
 3. Asses quality of assemblies
 4. Bin contigs into MAGs
 5. Asses completeness, contamination and strain heterogeniety of the MAGs

### Dataset 
The samples were taken from a biogas plant near Cologne over the course of 587 days. Each month samples were taken and analysed  based on 16S amplicon sequences by Martin Fisher (https://sfamjournals.onlinelibrary.wiley.com/doi/full/10.1111/1751-7915.13313). In addition to the amolicon sequencing, the samples were also sequenced for metagenomics. \
In this protocoll three exemplanary samples were analysed.

### HPC system
 All steps were run on the HPC-server of the CAU (CAU-cluster) via Bash scripts. \
 To access the HPC-server:
 ```
 ssh -X sunam???@caucluster.rz.uni-kiel.de
 ```
 The scripts were written in `Sublime Text` and executed though a Linux Shell.
```
sbatch <script.sh>
```
Example bash script:
```
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH --time=01:00:00
#SBATCH --job-name=example
#SBATCH --output=example.out
#SBATCH --error=example.err
#SBATCH --partition=all
#SBATCH --reservation=biol217

#activate environment
module load miniconda3/4.7.12.1
conda activate /home/sunam226/.conda/env/anvio
#Commandlines
example.command
#this prints the required resources into your logfile
jobinfo
```
For the course we were able to access the HPC with special persissions through the reservation command.\
Unless specified otherwise the commands were run in a miniconda environment.

 ### Tools used:
 | Tool | Version | Repository |
| --- | --- | --- |
| fastqc | 0.11.9 | [FastQC](https://github.com/s-andrews/FastQC ) |
| fastp | 0.22.0 | [fastp](https://github.com/OpenGene/fastp ) |
| megahit | 1.2.9 | [megahit](https://github.com/voutcn/megahit ) |
| samtools | 1.9 | [samtools](https://github.com/samtools/samtools ) |
| QUAST | 5.0.2 | [quast](https://quast.sourceforge.net/quast ) |
| Bowtie2 | 2.4.5 | [bowtie2](https://bowtie-bio.sourceforge.net/bowtie2/index.shtml ) |
| Concoct | 1.1.0 | [CONCOCT](https://github.com/BinPro/CONCOCT ) |
| MetaBAT2 | 2.12.1 | [Metabat2](https://bitbucket.org/berkeleylab/metabat/src/master/ ) |
| DASTool | 1.1.5 | [DAS_Tool](https://github.com/cmks/DAS_Tool ) |
| anvi´o | 7.1 | [anvi’o](https://anvio.org/ ) |
| GUNC | 1.0.5 | [GUNC](https://grp-bork.embl-community.io/gunc/ ) |

-----------------------------
## 1.1 Pre-processing of the raw reads
The raw read were copied ito my working directory
```
mkdir /work_beegfs/sunam236/day2

cp /home/sunam226/Day2/0_raw_reads/*.fastq.gz /work_beegfs/sunam236/day2
cd ./day2
ls
```
Quality of the reads was analysed with `FastQC` based on the phred quality score. 
```
for i in *.gz; do fastqc $i -o output_folder/; done
```
The sequences were trimmed with `Fastp`
```
for i in `ls *_R1.fastq.gz`;
do
    second=`echo ${i} | sed 's/_R1/_R2/g'`
    fastp -i ${i} -I ${second} -R _report -o ../clean_reads/"${i}" -O ../clean_reads/"${second}" -t 6 -q 20

done
```
--------------------------------------
## 1.2 Assembled reads into contigs/fasta
The processed data was assembled using `Megahit`.
```
cd /work_beegfs/sunam236/day2/clean_reads

megahit -1 BGR_130305_R1.fastq.gz -1 BGR_130527_R1.fastq.gz -1 BGR_130708_R1.fastq.gz -2 BGR_130305_R2.fastq.gz -2 BGR_130527_R1.fastq.gz -2 BGR_130708_R2.fastq.gz --min-contig-len 1000 --presets meta-large -m 0.85 -o ../megahit -t 20   
```
The contigs were transformde into FASTG files to visualized in `Bandage`. 
```
cd /day3/3_coassembly/

megahit_toolkit contig2fastg 99 final.contigs.fa > final.contigs.fastg       
```          
Open Bandage and Load the file

**<u>Question 1**: <br> 
Submit your generated figure and explain shortly what you can see </u>
![image](./images/Bandage_day3.png)
The picture shows the de novo created contigs from the processed reads
* The length of each line represents the length of the contig sequence
* Loops represent k-mers with multiple cintinuing options
* The colour is selected based on assumed MAG assignment
* The smaller fragments are reads, that were not able to be assigned to any of the longer contigs 
-------------------------------
## 1.3 Asses quality of assemblies
The quality assesment of the megahit results is performed with `Quast`. The output is given as PDF and html.
```
cd /work_beegfs/sunam236/3_coassembly/

metaquast -t 6 -o ../3_metaquast/ -m 1000 final.contigs.fa
```
*This step did not work due to a conda permission error, so the outpout was copied from a backup*

**<u>Question 2**: <br> 
What is your N50 value?</u>
* 2963 

<u>How many contigs are assembled?</u>
* 57414

<u>Whats is the total length of the contigs?</u>
* 145675865 Bp = 145,7 Mbp

----------------------------------
## 1.4 Bin contigs into MAGs
Binning of the contigs: <br>
To make the next steps easier the contig names were shortened *This step was performed directly in the console, without a bash script*
```
anvi-script-reformat-fasta final.contigs.fa -o /work_beegfs/sunam236/day3/contigs.anvio.fa --min-len 1000 --simplify-names--report-file name_conversion.txt
```
Afterwards the bins are mapped in a loop, so every final.contigs.fasta recieves a corresponding mapping file. 
```
module load bowtie2
cd ./2_fastp/

for i in  `ls *mapped_R1.fastq.gz`;
do
    second=`echo ${i} | sed 's/_R1/_R2/g'`
    bowtie2 --very-fast -x ../4_mapping/contigs.anvio.fa.index -1 ${i} -2 ${second} -S ../4_mapping/"$i".sam 
done
```
The output are sequence mapping files (.sam), which were converted to binary alignment and map files (.bam).
```
module load samtools

cd ./4_mapping/

for i in *.sam; do samtools view -bS $i > "$i".bam; done
```
#### Anvi'o
From here on `anvio` commands are used. <br> The **An**anlysis and **Vi**sualization plattform for microbial **'O**mics combines many of the computational strategies of data-enabled microbiology.

A contigs.db file was created, which contains key information associated with the sequences. <br> This command computes k-mer frequencies for each contig, soft-split contigs > 20000 bp into smaller ones and identif open reading frames using Prodigal.

```
cd /work_beegfs/sunam236/day3/

anvi-gen-contigs-database -f ./4_mapping/contigs.anvio.fa -o ./5_anvio_profiles/contigs.db -n 'biol217'
```
Afterwards a Hidden Markov Models (HMM) was run on the contigs, searching for specific single copy core genes with known functions. 
``` 
anvi-run-hmms -c ./5_anvio_profiles/contigs.db 
```

Visualizing the result with anvi'o interactive (directly in the console):
```
srun --pty --mem=10G --nodes=1 --tasks-per-node=1 --cpus-per-task=1 --partition=all /bin/bash
```
Note the accessed node
```
conda activate /home/sunam225/miniconda3/miniconda4.9.2/usr/etc/profile.d/conda.sh/envs/anvio-7.1

anvi-display-contigs-stats contigs.db
```
Open a new terminal, not logged into HPC
```
ssh -L 8060:localhost:8080 sunam236caucluster-old.rz.uni-kiel.de

ssh -L 8080:localhost:8080 node???
```
Open http://127.0.0.1:8060/ in your browser
#### Binning with Anvio
Sort and index the .bam files with `samtools` in anvi'o.
```
cd ./4_mapping/

for i in *.bam; do anvi-init-bam $i -o ../5_anvio_profiles/"$i".sorted.bam; done
```
Create an anvio profile to store sample-specific information about the contigs in a single profile. <br> It processes only contigs >2500 nt.<br>
Processing includes:
* Recovery of mean coverage, SD of coverage and average coverage for the inner quartiles.
* Characterization of single nucleotide variants (SNVs)
```
cd ./5_anvio_profiles

for i in `ls *.sorted.bam | cut -d "." -f 1`; do anvi-profile -i "$i".fastq.gz.sam.bam.sorted.bam -c contigs.db -o ../6_profiling/$i; done
```
This created a folder containing profiles.db and a .txt log.<br>

In the next steps all profiles of the different samples were merched into one:
```
cd ./6_profiling/

anvi-merge ./BGR_130305_mapped_R1/PROFILE.db ./BGR_130527_mapped_R1/PROFILE.db ./BGR_130708_mapped_R1/PROFILE.db -o ./ -c ../5_anvio_profiles/contigs.db --enforce-hierarchical-clustering
```
In this course two different Binners were used: `Metabat2` and `Concoct`. The result of both Binners were consolidated using `DASTool`. *The binning steps did not work, so the files were copied from a back up*

###### Binning with Metabat2
```
cd ./6_profiling/

anvi-merge ./BGR_130305_mapped_R1/PROFILE.db ./BGR_130527_mapped_R1/PROFILE.db ./BGR_130708_mapped_R1/PROFILE.db -o ./ -c ../5_anvio_profiles/contigs.db --enforce-hierarchical-clustering
```
###### Binning with CONCOCT
```
anvi-cluster-contigs -p /PATH/TO/merged_profiles/PROFILE.db -c /PATH/TO/contigs.db -C consolidated_bins --driver dastool -T 20 --search-engine diamond -S METABAT,CONCOCT --log-file log_consolidation_of_bins --just-do-it

anvi-summarize -p /PATH/TO/merged_profiles/PROFILE.db -c /PATH/TO/contigs.db -o /PATH/TO/SUMMARY_consolidated_bins -C consolidated_bins
```
###### Consolidating bins with DASTool
```
anvi-cluster-contigs -p /PATH/TO/merged_profiles/PROFILE.db -c /PATH/TO/contigs.db -C consolidated_bins --driver dastool -T 20 --search-engine diamond -S METABAT,CONCOCT --log-file log_consolidation_of_bins --just-do-it

anvi-summarize -p /PATH/TO/merged_profiles/PROFILE.db -c /PATH/TO/contigs.db -o /PATH/TO/SUMMARY_consolidated_bins -C consolidated_bins
```
*From here the previously mentioned copied files were used to awnser the questions and proceed with the quality estimation.*

**<u>Question 3**: <br> 
* Number of Archaea bins from MetaBAT2: </u><br>
  3
* <u>Number of Archaea bins from CONCOCT:</u><br>
  2
* <u>Numer of Archaea bins from consolidating:</u><br>
  2
### MAGs quality estimation
See the amount of bins
```
anvi-estimate-genome-completeness -p PROFILE.db -c contigs.db --list-collection
```
Visualize it through anvi-interactive (directly in the console)
```
srun --pty --mem=10G --nodes=1 --tasks-per-node=1 --cpus-per-task=1 --partition=all /bin/bash
```
Note the accessed node
```
conda activate /home/sunam225/miniconda3/miniconda4.9.2/usr/etc/profile.d/conda.sh/envs/anvio-7.1

anvi-interactive -p ./PROFILE.db -c ./contigs.db -C ../../consolidated_bins
```
open a new terminal, not logged into CAUcluster
```
ssh -L 8060:localhost:8080 sunam236@caucluster-old.rz.uni-kiel.de
ssh -L 8080:localhost:8080 node010
```
Open in a Browser: http://127.0.0.1:8060/

![image](images/Collection__consolidated_bins__for_merged_profiles.svg)

**<u>Question 4**: <br> 
* Which binning strategy gives the best quality archea bins:</u><br>
  dastool (consolidated) gives the best quality, which was expected since it it not a binner itself and just merges the results of the two binners. MEtabat2 seems to be better than CONCOCT for our data.
* <u>How many archaea bins do you get of high quality</u><br>
  2

Check the binning quality for all three. Result is saved in working directory
```
anvi-estimate-genome-completeness -c ./contigs.db -p ./PROFILE.db -C consolidated_bins > genome_completeness_dastool

anvi-estimate-genome-completeness -c ./contigs.db -p ./PROFILE.db -C METABAT > genome_completeness_metabat2

anvi-estimate-genome-completeness -c ./contigs.db -p ./PROFILE.db -C CONCOCT > genome_completeness_concoct
```

### Bin refinement
From this point only the archea bins are used. <br>
Create a summary folder with *anvi-summarize* containing a comprehensive overview of the collection and statistics created by anvio.
```
anvi-summarize -c ./contigs.db -p ./5_anvio_profiles/merged_profiles/PROFILE.db -C consolidated_bins -o ./5_anvio_profiles/summary --just-do-it
```
Copy the archea bins to a seperate folder
```
cd ./5_anvio_profiles/summary/bin_by_bin

mkdir ../../archea_bin_refinement

cp ./Bin_Bin_1_sub/*.fa ../../archea_bin_refinement/

cp ./Bin_METABAT__25/*.fa ../../archea_bin_refinement/
```
---------------------------------
## 1.5 Asses completeness, contamination and strain heterogeniety of the MAGs

### Chimera detection
**G**genome **UNC**cluttered (`GUNC`) was used to detect chimeric MAGs.

```
conda activate /home/sunam226/gunc

cd ../../archea_bin_refinement
mkdir GUNC

cd /work_beegfs/sunam236/Day5/5_anvio_profiles/archea_bin_refinement

for i in *.fa; do gunc run -i "$i" -r /home/sunam226/Databases/

gunc_db_progenomes2.1.dmnd --out_dir GUNC --threads 10 --detailed_output; done
```
look at the output file (.tsv) to see if a MAG is chimeric or not (clade seperation score)


**<u>Question 5**: <br> 
* Do you get Archea bins that are chimeric:</u><br>
  *clade seperation index close to one = chimeric* <br>
metabat archea = not chmieric <br>
* species level =0.9 <br>
Concoct archea =  <br>
* kingdom, phylum, class = chimeric
* After class it isnt marked as chimeric, because the groups have not been named yet
* <u>Explain what a chimeric bin is in your own words</u><br>
  A chimeric bin contains a MAG that is created from more than one genome. So a mixture of at least two different species

### Manual bin refinement
*Only for the non chimeric bins*
Remove the redundant parts through the interactive anvio interface, by pre-selecting the better MAGs (<70% completeness)
`Before starting, create a copy/backup of the unrefinde bins in ARCHEA_BIN_REFINEMENT.` The manual refinement will overwrite the unrefined files.
```
srun --reservation=biol217 --pty --mem=10G --nodes=1 --tasks-per-node=1 --cpus-per-task=1 /bin/bash
```
Note the accessed node
```
anvi-refine -c ./4_mapping/contigs.db -C consolidated_bins -p ./5_anvio_profiles/merged_profiles/PROFILE.db --bin-id Bin_METABAT__25
```
open a new terminal, not logged into CAUcluster
```
ssh -L 8060:localhost:8080 sunam236@caucluster-old.rz.uni-kiel.de
ssh -L 8080:localhost:8080 node010
```
Open in a browser:  http://127.0.0.1:8060/

**<u>Question 6**: <br> 
* Does the quality of your Archea
 improve?:</u><br>
    * Before: Comp.:97.4   Red.:5.3
    * After:  Comp.:93.4   Red.:5.3
    * after removing some bins clustered by difference, the composition reduced but the redundancy stayed the same. So visually the quality increases, but the redundancy score did not reduce.
* <u>Explain what a chimeric bin is in your own words</u><br>
  A chimeric bin contains a MAG that is created from more than one genome. So a mixture of at least two different species
![image](images/Refining_Bin_METABAT__25_from__consolidated_bins.svg)

**<u>Question 7**: <br> 
* How abundant are the archea bins in the 3 samples</u><br>
  * Metabat: 1.76 | 1.14 | 0.58
  * Concoct: 0.96 | 0.00 | 0.40
* <u>you can also use anvi-inspect -p -c, anvi-script-get-coverage-from-bam or, anvi-profile-blitz. Please look up the help page for each of those commands and construct the appropriate command line</u><br>

### Taxonomic assignment
Add taxonomic annotations to the MAGs through the single copy core genes (SCGs)
```
anvi-run-scg-taxonomy -c ./4_mapping/contigs.db -T 20 -P 2

anvi-estimate-scg-taxonomy -c ./4_mapping/contigs.db --metagenome-mode
```
estimate the abundance of rRNAs in  the dataset
```
anvi-estimate-scg-taxonomy -c ./4_mapping/contigs.db -p ./5_anvio_profiles/merged_profiles/PROFILE.db --metagenome-mode --compute-scg-coverages --update-profile-db-with-taxonomy
```
Save the output from the terminal:
```
anvi-estimate-scg-taxonomy -c ./4_mapping/contigs.db -p ./5_anvio_profiles/merged_profiles/PROFILE.db --metagenome-mode --compute-scg-coverages --update-profile-db-with-taxonomy > temp.txt
```
Recieve one final summary of all the info from the consolidated bins
```
anvi-summarize -c ./contigs.db -p ./5_anvio_profiles/merged_profiles/PROFILE.db -C consolidated_bins -o ./5_anvio_profiles/summary2 --just-do-it
```
Rename bins_summary.txt to .tsv and open with libre office


DAY6!!!!!!!!!!!!!